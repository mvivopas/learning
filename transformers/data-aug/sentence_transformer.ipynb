{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation for Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"/Users/mariavivo/repos/meri/learning/transformers/data-aug/data-aug-sentence-transf.png\" width=\"500\" height=\"300\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env transformers\n",
    "import datasets\n",
    "\n",
    "# load dataset\n",
    "stsb = datasets.load_dataset('glue', 'stsb', split='train')\n",
    "stsb_dev = datasets.load_dataset('glue', 'stsb', split='validation')\n",
    "stsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data = []\n",
    "for row in stsb:\n",
    "    train_data.append(\n",
    "        InputExample(\n",
    "            texts=[row['sentence1'], row['sentence2']],\n",
    "            label=int(float(row['label']))\n",
    "        )\n",
    "    )\n",
    "\n",
    "batch_size = 16\n",
    "# load our training data (first 95%) into a dataloader\n",
    "loader = DataLoader(\n",
    "    train_data, shuffle=True, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "# initialize cross-encoder\n",
    "cross_encoder = CrossEncoder('bert-base-uncased', num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune cross-encoder\n",
    "num_epochs = 1\n",
    "warmup = int(len(loader) * num_epochs * 0.4)\n",
    "\n",
    "cross_encoder.fit(\n",
    "    train_dataloader=loader,\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup,\n",
    "    output_path='bert-stsb-cross-encoder'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***The number of warmup steps is 40% of the total training steps. It is high but helps prevent overfitting.*** \n",
    "\n",
    "* ***The same could likely be achieved using a lower learning rate (the default is 2e-5)***\n",
    "\n",
    "_Evaluation of the cross-encoder model on the dev set returns a correlation score of 0.578_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gold = datasets.load_dataset('glue', 'stsb', split='train')\n",
    "\n",
    "gold = pd.DataFrame({\n",
    "    'sentence1': gold['sentence1'],\n",
    "    'sentence2': gold['sentence2']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# initialize a new pairs dataframe, loop through each unique \n",
    "# sentence from the sentence1 column and find new pairs from \n",
    "# the sentence2 column.\n",
    "\n",
    "pairs = pd.DataFrame()\n",
    "# loop through each unique sentence in 'sentence1'\n",
    "for sentence1 in tqdm(list(set(gold['sentence1']))):\n",
    "    # get a sample of 5 rows that do not contain the current 'sentence1'\n",
    "    sampled = gold[gold['sentence1'] != sentence1].sample(5)\n",
    "    # get the 5 sentence2 sentences\n",
    "    sampled = sampled['sentence2'].tolist()\n",
    "    for sentence2 in sampled:\n",
    "        # append all of these new pairs to the new 'pairs' dataframe\n",
    "        pairs = pairs.append({\n",
    "            'sentence1': sentence1,\n",
    "            'sentence2': sentence2\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# remove duplicates\n",
    "pairs = pairs.drop_duplicates()\n",
    "print(f\"Now there are {len(pairs)} unlabeled sentence pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling the Silver Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously fine-tuned cross-encoder\n",
    "cross_encoder = CrossEncoder('bert-stsb-cross-encoder')\n",
    "\n",
    "# predict labels\n",
    "#--------------------------------------------------------------\n",
    "# zip pairs together in format for the cross-encoder\n",
    "silver = list(zip(pairs['sentence1'], pairs['sentence2']))\n",
    "# predict labels for the unlabeled silver data\n",
    "scores = cross_encoder.predict(silver)\n",
    "\n",
    "# add the predicted scores to the pairs dataframe\n",
    "pairs['label'] = scores.tolist()\n",
    "pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training, we need to merge the silver and gold datasets\n",
    "all_data = gold.append(pairs, ignore_index=True)\n",
    "\n",
    "# format into input examples\n",
    "train = []\n",
    "for _, row in all_data.iterrows():\n",
    "    train.append(\n",
    "        InputExample(\n",
    "            texts=[row['sentence1'], row['sentence2']],\n",
    "            label=float(row['label'])\n",
    "        )\n",
    "    )\n",
    "\n",
    "# initialize dataloader\n",
    "loader = DataLoader(\n",
    "    train, shuffle=True, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model consists of a:\n",
    "- ***core transformer model*** ( `bert-base-uncased` )\n",
    "- ***pooling layer*** to _transform the 512 token-level vectors into single sentence vectors_ (`mean pooling method`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import models, SentenceTransformer\n",
    "from sentence_transformers import losses\n",
    "\n",
    "# initialize model\n",
    "bert = models.Transformer('bert-base-uncased')\n",
    "pooler = models.Pooling(\n",
    "    bert.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True\n",
    ")\n",
    "model = SentenceTransformer(modules=[bert, pooler])\n",
    "\n",
    "# define loss function -> Cosine Similarity\n",
    "# (to optimize similarity scores is nice func)\n",
    "loss = losses.CosineSimilarityLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and training\n",
    "# We use the default learning rate and warmup for the first 15% of steps\n",
    "epochs = 1\n",
    "# warmup for first 15% of training steps\n",
    "warmup_steps = int(len(loader) * epochs * 0.15)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(loader, loss)],\n",
    "    epochs=epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='bert-stsb-aug'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"/Users/mariavivo/repos/meri/learning/transformers/data-aug/results-data-aug-sentence-transf.png\" width=\"500\" height=\"300\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
